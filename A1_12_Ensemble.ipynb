{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba56020c-3490-4d59-9834-34a6ff61a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, default_collate\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "#### LOADING THE MODEL\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "TOKEN = \"REDACTED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c05aee1-1c5a-44af-9880-e189426b40e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - Model loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#### LOADING THE MODEL\n",
    "resnet_model = resnet18(pretrained=False)\n",
    "resnet_model.fc = torch.nn.Linear(512, 44)\n",
    "\n",
    "try:\n",
    "    ckpt = torch.load(\"./01_MIA.pt\", map_location=\"cpu\", weights_only=False)\n",
    "    resnet_model.load_state_dict(ckpt)\n",
    "    print(\"Success - Model loaded\")\n",
    "except Exception as error:\n",
    "    print(f\"Error loading model: {error}\")\n",
    "    exit(1)\n",
    "\n",
    "resnet_model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c49ed374-ffc7-4788-b09b-dde13da216c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DATASETS\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.ids = []\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int]:\n",
    "        id_ = self.ids[index]\n",
    "        img = self.imgs[index]\n",
    "        if not self.transform is None:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[index]\n",
    "        return id_, img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "class MembershipDataset(TaskDataset):\n",
    "    def __init__(self, transform=None):\n",
    "        super().__init__(transform)\n",
    "        self.membership = []\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int, int]:\n",
    "        id_, img, label = super().__getitem__(index)\n",
    "        return id_, img, label, self.membership[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f21c09d2-0b26-47da-86b9-27ea411959d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public dataset: 20000 samples\n",
      "  Members: 10000, Non-members: 10000\n",
      "Private dataset: 20000 samples\n"
     ]
    }
   ],
   "source": [
    "# Public Data\n",
    "try:\n",
    "    public_data : MembershipDataset = torch.load(\"./pub.pt\", map_location=\"cpu\", weights_only=False)\n",
    "    print(f\"Public dataset: {len(public_data)} samples\")\n",
    "    members = sum(public_data.membership)\n",
    "    print(f\"  Members: {members}, Non-members: {len(public_data) - members}\")\n",
    "except Exception as error:\n",
    "    print(f\"Error: Could not load public dataset - {error}\")\n",
    "    exit(1)\n",
    "\n",
    "# Private Data\n",
    "try:\n",
    "    private_data : MembershipDataset = torch.load(\"./priv_out.pt\", map_location=\"cpu\", weights_only=False)\n",
    "    print(f\"Private dataset: {len(private_data)} samples\")  \n",
    "except Exception as error:\n",
    "    print(f\"Error: Could not load private dataset - {error}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3f6290-a33a-4ba2-af1f-191dc76db563",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS\n",
    "\n",
    "def print_auc_tpr(y, probs):\n",
    "    auc = roc_auc_score(y, probs)\n",
    "\n",
    "    # TPR @ FPR=0.05\n",
    "    fpr, tpr, _ = roc_curve(y,probs)\n",
    "    idx = (fpr >= 0.05).nonzero()[0][0] if any(fpr >= 0.05) else -1\n",
    "    tpr_at_fpr = tpr[idx] if idx != -1 else 0.0\n",
    "\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  TPR@FPR=0.05: {tpr_at_fpr:.4f}\\n\")\n",
    "\n",
    "def print_attack_stats(model_name, score):\n",
    "    print(f\"\\nAttack Results : {model_name}\")\n",
    "    print(f\"  Generated scores for {len(score)} samples\")\n",
    "    print(f\"  Score range: [{np.min(score):.4f}, {np.max(score):.4f}]\")\n",
    "    print(f\"  Score mean: {np.mean(score):.4f}\")\n",
    "    print(f\"  Score std: {np.std(score):.4f}\")\n",
    "\n",
    "def export_scores_csv(model_name, ids, scores):\n",
    "    df = pd.DataFrame({\n",
    "        \"ids\": ids,\n",
    "        \"score\": scores,\n",
    "    })\n",
    "    \n",
    "    submission_file = f\"submission_{model_name}.csv\"\n",
    "    df.to_csv(submission_file, index=False)\n",
    "    print(f\"\\nSubmission saved to: {submission_file}\")\n",
    "\n",
    "# Take in only Image, Label, Membership fields - No Need of ID field\n",
    "def collate_fn_public(batch):\n",
    "    filtered_batch = []\n",
    "    for sample in batch:\n",
    "        filtered_batch.append(sample[1:])\n",
    "    return default_collate(filtered_batch)\n",
    "\n",
    "# Take in only Image, Label fields - No Need of ID and Membership Field\n",
    "def collate_fn_private(batch):\n",
    "    filtered_batch = []\n",
    "    for sample in batch:\n",
    "        filtered_batch.append(sample[1:3])\n",
    "    return default_collate(filtered_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078b44ea-0f4c-464e-9b1c-082dbad77d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add this as a transofrmation to pre-process the images\n",
    "mean = [0.2980, 0.2962, 0.2987]\n",
    "std = [0.2886, 0.2875, 0.2889]\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),           \n",
    "    transforms.ToTensor(),            \n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Transform datasets w.r.t the transformation Parameters\n",
    "public_data.transform = transform\n",
    "private_data.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44814dd4-5874-4e96-9f62-90500cbc6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features - Entropy, Confidence, Logit, LiRA information.\n",
    "\n",
    "def extract_mia_lira_features(dataset, is_public=True):\n",
    "    \"\"\"\n",
    "    This function extracts set of features including softmax probabilities,\n",
    "    cross-entropy losses, entropy-based measures, logits, and various confidence scores. \n",
    "    Also computes the conf_member, conf_non_member on when is_public is True\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : torch.utils.data.Dataset\n",
    "        The dataset to extract features from. Should contain images, labels, and optionally\n",
    "        membership indicators (if `is_public=True`).\n",
    "\n",
    "    is_public : bool, optional (default=True)\n",
    "        Indicates whether the dataset is a public dataset (with membership labels). \n",
    "        Collate functions are loaded accordingly\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : np.ndarray, shape (n_samples, n_features)\n",
    "        The extracted feature vectors for all samples. Includes probabilities, loss values,\n",
    "        entropy metrics, prediction confidence margins, and logit-based metrics.\n",
    "\n",
    "    conf_member : list of float\n",
    "        A list of true class probabilities for samples labeled as members (only returned\n",
    "        if `is_public=True`).\n",
    "\n",
    "    conf_non_member : list of float\n",
    "        A list of true class probabilities for samples labeled as non-members (only returned\n",
    "        if `is_public=True`).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    Feature vector (per sample) includes:\n",
    "        - Softmax probabilities for each class\n",
    "        - Cross-entropy loss\n",
    "        - Total class entropy\n",
    "        - Adjusted entropy (entropy scaled by 1 - true class prob)\n",
    "        - Top-2 confidence margin\n",
    "        - True class probability\n",
    "        - Prediction correctness (0 or 1)\n",
    "        - Max logit\n",
    "        - True class logit\n",
    "        - Gradient proxy (|1 - true logit|)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    conf_member = []\n",
    "    conf_non_member = []   \n",
    "    \n",
    "    if is_public:\n",
    "        dataloader = DataLoader(dataset, batch_size=64, shuffle=False, collate_fn=collate_fn_public)\n",
    "\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=64, shuffle=False, collate_fn=collate_fn_private)\n",
    "    \n",
    "    print(f\"Extracting features from {len(dataset)} samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "         for batch in dataloader:\n",
    "            if is_public:\n",
    "                batch_images, batch_labels, batch_memberships = batch\n",
    "                            \n",
    "            else:\n",
    "                batch_images, batch_labels = batch\n",
    "            \n",
    "            # Get model predictions - logits\n",
    "            logits = resnet_model(batch_images)\n",
    "            soft_max_probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            ## FEATURE EXTRACTION\n",
    "\n",
    "            # Cross Entropy Loss per sample\n",
    "            sample_loss = F.cross_entropy(logits, batch_labels, reduction='none')\n",
    "\n",
    "            # Cross Entropy for predicted classes\n",
    "            class_entropy = -torch.sum(soft_max_probs * torch.log(soft_max_probs + 1e-10), dim=1)\n",
    "\n",
    "            # Adjusted Entropy for MIA\n",
    "            max_confidence, prediction = torch.max(soft_max_probs, dim=1)\n",
    "            true_class_prob = soft_max_probs[torch.arange(batch_labels.size(0)), batch_labels]\n",
    "            adjusted_entropy = class_entropy * (1 - true_class_prob)\n",
    "\n",
    "            # Correctness of prediction\n",
    "            correctness = (prediction == batch_labels).float()\n",
    "             \n",
    "            # Top-2 confidence margin\n",
    "            sorted_probs, _ = torch.sort(soft_max_probs, dim=1, descending=True)\n",
    "            top2_conf_margin = sorted_probs[:, 0] - sorted_probs[:, 1]\n",
    "            \n",
    "            # Logit features\n",
    "            max_logit = torch.max(logits, dim=1).values\n",
    "            true_logit = logits[torch.arange(batch_labels.size(0)), batch_labels]\n",
    "            gradient_proxy = torch.abs(1 - true_logit)\n",
    "\n",
    "            # Create Batch Feature Vector - [64,52]\n",
    "            batch_features = np.hstack([\n",
    "                soft_max_probs.numpy(),           # [64,44]\n",
    "                sample_loss.numpy().reshape(-1,1), #[64,1]\n",
    "                class_entropy.numpy().reshape(-1,1), \n",
    "                adjusted_entropy.numpy().reshape(-1,1),\n",
    "                top2_conf_margin.numpy().reshape(-1,1), \n",
    "                true_class_prob.numpy().reshape(-1,1),\n",
    "                correctness.numpy().reshape(-1,1), \n",
    "                max_logit.numpy().reshape(-1,1), \n",
    "                true_logit.numpy().reshape(-1,1), \n",
    "                gradient_proxy.numpy().reshape(-1,1)\n",
    "            ]) \n",
    "\n",
    "            # Append results to features\n",
    "            features.append(batch_features)\n",
    "             \n",
    "            if is_public:\n",
    "                # Collect confidences for LiRA training\n",
    "                for i, member in enumerate(batch_memberships):\n",
    "                    prob = true_class_prob[i].item()\n",
    "                    if member == 1:\n",
    "                        conf_member.append(prob)\n",
    "                    elif member == 0:\n",
    "                        conf_non_member.append(prob)\n",
    "                        \n",
    "          \n",
    "   # After loop: concatenate all batch feature lists into feature numpy array\n",
    "    features = np.vstack(features)\n",
    "\n",
    "    \n",
    "    return (features, conf_member, conf_non_member) if is_public else features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdedc51a-cbbc-4ec8-b892-30a1f16b20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiRA Based Model\n",
    "def compute_lira_scores(dataset, conf_member, conf_non_member):\n",
    "    \"\"\"\n",
    "    Compute LiRA (Likelihood Ratio Attack) membership scores for a private dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : torch.utils.data.Dataset\n",
    "        A private dataset whose samples will be scored for membership inference.\n",
    "\n",
    "    conf_member : list of float\n",
    "        List of predicted confidence values (true class probabilities) for known members in the public set.\n",
    "\n",
    "    conf_non_member : list of float\n",
    "        List of predicted confidence values (true class probabilities) for known non-members in the public set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lira_scores : np.ndarray of shape (n_samples,)\n",
    "        Membership scores (between 0 and 1) for each sample in the private dataset. .\n",
    "\n",
    "    \"\"\"\n",
    "    lira_scores = []\n",
    "    \n",
    "    # Loading a Public Dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False, collate_fn=collate_fn_private)\n",
    "\n",
    "    # Compute Gaussian parameters for LiRA\n",
    "    mean_member, std_member = np.mean(conf_member), np.std(conf_member) + 1e-8\n",
    "    mean_non_member, std_non_member = np.mean(conf_non_member), np.std(conf_non_member) + 1e-8\n",
    "\n",
    "    # Apply LiRA scoring to private data\n",
    "    lira_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch_images, batch_labels = batch\n",
    "            \n",
    "            logits = resnet_model(batch_images)\n",
    "            soft_max_probs = F.softmax(logits, dim=1)\n",
    "            true_class_prob = soft_max_probs[torch.arange(batch_labels.size(0)), batch_labels]\n",
    "    \n",
    "            for i in range(len(batch_labels)):\n",
    "                prob = true_class_prob[i].item()\n",
    "    \n",
    "                # LiRA likelihood ratio scoring\n",
    "                likelihood_member = norm.pdf(prob, mean_member, std_member)\n",
    "                likelihood_non_member = norm.pdf(prob, mean_non_member, std_non_member)\n",
    "\n",
    "                # Avoid division by zero using e^-8\n",
    "                likelihood_ratio = likelihood_member / (likelihood_non_member + 1e-8)\n",
    "                membership_prob = likelihood_ratio / (1 + likelihood_ratio)\n",
    "    \n",
    "                lira_scores.append(membership_prob)\n",
    "\n",
    "    \n",
    "    return np.array(lira_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "819bfeba-fc21-4193-b16a-3ac638f88c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mia_lira_model(X, y, model):\n",
    "    \"\"\"\n",
    "    Train and calibrate a Membership Inference Attack (MIA) model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray of shape (n_samples, n_features)\n",
    "        Feature matrix used for training and calibration.\n",
    "        \n",
    "    y : np.ndarray of shape (n_samples,)\n",
    "        Binary target labels indicating membership (1 = member, 0 = non-member).\n",
    "\n",
    "    model : sklearn-like classifier\n",
    "        Any classifier that implements `fit`, `predict`, and `predict_proba`. This will serve as the\n",
    "        base model for MIA (e.g., RandomForest, LightGBM, or MLP).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    calibrated_clf : sklearn.calibration.CalibratedClassifierCV\n",
    "        A probability-calibrated classifier using isotonic regression based on validation performance.\n",
    "        The calibrated model can now return better-calibrated membership probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split for calibration\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train base model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Calibrate\n",
    "    calibrated_clf = CalibratedClassifierCV(estimator=model, method='isotonic', cv='prefit')\n",
    "    calibrated_clf.fit(X_val, y_val)\n",
    "\n",
    "    y_score = calibrated_clf.predict_proba(X_val)[:, 1]\n",
    "    print_auc_tpr(y_val, y_score)\n",
    "\n",
    "    return calibrated_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc87085-97db-4bd8-baf1-a3da35546015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_attack(X, y):\n",
    "    \"\"\"\n",
    "    Train an ensemble of machine learning models (Random Forest, LightGBM, MLP) \n",
    "    to perform a Membership Inference Attack (MIA).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray of shape (n_samples, n_features)\n",
    "        The feature matrix extracted from the victim model. Each row corresponds to one sample and contains\n",
    "        features such as softmax probabilities, entropy, margin, logit values, etc.\n",
    "\n",
    "    y : np.ndarray of shape (n_samples,)\n",
    "        The ground truth binary labels indicating membership status. 1 for member, 0 for non-member.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        A dictionary with model names as keys and trained model instances as values.\n",
    "        Format:\n",
    "        {\n",
    "            \"RF\": {\"model\": RandomForestClassifier instance},\n",
    "            \"LightGBM\": {\"model\": LGBMClassifier instance},\n",
    "            \"MLP\": {\"model\": MLPClassifier instance}\n",
    "        }\n",
    "    \"\"\"\n",
    "    models = { \n",
    "        \"RF\": RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=15, min_samples_split=5,\n",
    "            random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \"LightGBM\": LGBMClassifier(\n",
    "            n_estimators=200, max_depth=15, learning_rate=0.05,\n",
    "            random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \"MLP\": MLPClassifier(\n",
    "            hidden_layer_sizes=(64, 32), max_iter=100, random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "     # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    # Float32 instead of Float64\n",
    "    X_scaled = scaler.fit_transform(X).astype(np.float32)  \n",
    "\n",
    "    # Float32 instead of Int64\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"🔹 Training: {name}\")\n",
    "        \n",
    "        results[name] = {}\n",
    "\n",
    "        results[name][\"model\"] = train_mia_lira_model(X_scaled, y, model)\n",
    "\n",
    "        results[name] = {\"model\": model}\n",
    " \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4751e8-c695-4b88-b579-12a386974a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from 20000 samples...\n",
      "Computing LiRA Scores on Public Dataset...\n",
      "🔹 Training: RF\n",
      "  AUC: 0.6715\n",
      "  TPR@FPR=0.05: 0.1454\n",
      "\n",
      "🔹 Training: LightGBM\n",
      "[LightGBM] [Info] Number of positive: 8005, number of negative: 7995\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12423\n",
      "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 54\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500313 -> initscore=0.001250\n",
      "[LightGBM] [Info] Start training from score 0.001250\n",
      "  AUC: 0.6660\n",
      "  TPR@FPR=0.05: 0.1639\n",
      "\n",
      "🔹 Training: MLP\n",
      "  AUC: 0.6591\n",
      "  TPR@FPR=0.05: 0.1514\n",
      "\n",
      "Extracting features from 20000 samples...\n",
      "Computing LiRA Scores on Private Dataset...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# LiRA Based Model + Feature Extraction\n",
    "\n",
    "# Extract features from public data (for training)\n",
    "public_features, conf_member, conf_non_member= extract_mia_lira_features(public_data)\n",
    "\n",
    "# Extract LiRA scores from public data (for training)\n",
    "print(\"Computing LiRA Scores on Public Dataset...\")\n",
    "public_lira_scores = compute_lira_scores(public_data, conf_member, conf_non_member)\n",
    "\n",
    "# Combine features with LiRA\n",
    "combined_features = np.concatenate([public_features, public_lira_scores.reshape(-1, 1)], axis=1)\n",
    "\n",
    "# Extract memberships from public data (for training)\n",
    "public_memberships = np.array([public_data.membership[i] for i in range(len(public_data))])\n",
    "\n",
    "# Train attack model\n",
    "# attack_models = train_mia_lira_attack(combined_features, public_memberships)\n",
    "attack_models = ensemble_attack(combined_features, public_memberships)\n",
    "\n",
    "# Extract features from private data (for attack)\n",
    "private_features = extract_mia_lira_features(private_data, is_public=False)\n",
    "\n",
    "# Extract LiRA scores from public data (for training)\n",
    "print(\"Computing LiRA Scores on Private Dataset...\")\n",
    "private_lira_scores = compute_lira_scores(private_data, conf_member, conf_non_member)\n",
    "\n",
    "# Combine features with LiRA\n",
    "combined_features = np.concatenate([private_features, private_lira_scores.reshape(-1, 1)], axis=1)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "combined_features_scaled = scaler.fit_transform(combined_features).astype(np.float32)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5743503e-2ed5-4791-ab4d-7a85ff943104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting MIA-LiRA Scores on Private Dataset...\n",
      "\n",
      "Attack Results : RF\n",
      "  Generated scores for 20000 samples\n",
      "  Score range: [0.0073, 0.6209]\n",
      "  Score mean: 0.4450\n",
      "  Score std: 0.1227\n",
      "\n",
      "Submission saved to: submission_RF.csv\n",
      "\n",
      "Attack Results : LightGBM\n",
      "  Generated scores for 20000 samples\n",
      "  Score range: [0.0007, 0.8410]\n",
      "  Score mean: 0.3095\n",
      "  Score std: 0.1406\n",
      "\n",
      "Submission saved to: submission_LightGBM.csv\n",
      "\n",
      "Attack Results : MLP\n",
      "  Generated scores for 20000 samples\n",
      "  Score range: [0.0000, 0.9997]\n",
      "  Score mean: 0.5090\n",
      "  Score std: 0.1849\n",
      "\n",
      "Submission saved to: submission_MLP.csv\n"
     ]
    }
   ],
   "source": [
    "# Perform attack on private data\n",
    "print(\"Predicting MIA-LiRA Scores on Private Dataset...\")\n",
    "\n",
    "for model_name in attack_models.keys():\n",
    "\n",
    "    attack_models[model_name][\"score\"] = attack_models[model_name][\"model\"].predict_proba(combined_features_scaled)[:, 1]\n",
    "    \n",
    "    print_attack_stats(model_name, attack_models[model_name][\"score\"])\n",
    "\n",
    "    export_scores_csv(model_name, private_data.ids, attack_models[model_name][\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9afdf2c-0299-4fcf-9c68-219dbf00cde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detail': 'Exceeded submissions. Only 1/h allowed.'}\n"
     ]
    }
   ],
   "source": [
    "## Submit only when sure\n",
    "response = requests.post(\"http://34.122.51.94:9090/mia\", files={\"file\": open(\"submission_MLP.csv\", \"rb\")}, headers={\"token\": TOKEN})\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS Kernel",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
